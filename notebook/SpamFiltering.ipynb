{"cells":[{"cell_type":"markdown","id":"7b9bef79","metadata":{},"source":"# **Spam Filtering**\n\nThis notebook will deal with spam filtering using built-in modules and functions in JABA."},{"cell_type":"markdown","id":"8d891a12-ed02-4d20-8861-e0f8d7f31bf2","metadata":{},"source":"## Imports y Parametros"},{"cell_type":"code","execution_count":1,"id":"cdb93129","metadata":{},"outputs":[],"source":"import datetime\nfrom datetime import timedelta\n\nimport os\nimport re\n\nimport snscrape.modules.twitter as snstwitter\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\nimport matplotlib.pyplot as plt"},{"cell_type":"markdown","id":"b3216cd1-6de3-420d-9707-a9cc4633e4a8","metadata":{},"source":"We will pick dates alredy scrapped to work with."},{"cell_type":"code","execution_count":2,"id":"7be423b0-c422-41cd-add6-069aa9ce1886","metadata":{},"outputs":[],"source":"date_init = \"2014-01-01\"\ndate_limit = \"2014-05-10\"\n\ndate_from = datetime.datetime.strptime(date_init, '%Y-%m-%d').date()\ndate_until = datetime.datetime.strptime(date_limit, '%Y-%m-%d').date()"},{"cell_type":"markdown","id":"07a3fab3-aae3-45dc-ab9d-49f52eed160b","metadata":{},"source":["## **Read Databases**"]},{"cell_type":"markdown","id":"56b9ac58-8e46-4747-a34f-0b09acad857e","metadata":{},"source":"We define the working directory."},{"cell_type":"code","execution_count":3,"id":"55d9f863-6f77-4b42-a164-e7ee56a677cd","metadata":{},"outputs":[],"source":"t_path = \"../JABA/data/tweets\"\nt_file = \"tweet_list.csv\"\ns_file = \"tweet_sentiment_nltk.csv\""},{"cell_type":"markdown","id":"db108986-70a6-4b79-9d23-110ab8cbfa16","metadata":{},"source":"To pick the positive and negative emotions, we will use emojis, which represent in a way the sentiment of a text. They aren't perfect, but we should get good results."},{"cell_type":"code","execution_count":4,"id":"2eb94cb7-0d5f-42a4-bc0c-10c39b85f7bd","metadata":{},"outputs":[],"source":"positive_text = [':\\)', ':D', '=D','=\\)','😊', '🚀', '🔥','😋', '💰', '📈','💯']\nnegative_text = [':\\(','=\\(', ':c', ':C', '☹️', '😢', '😭', '🙁', '😟', '😒', '😔','📉','💀']\n\npositive_text_f = [':)', ':D', '=D','=)','😊', '🚀', '🔥','😋', '💰', '📈','💯']\nnegative_text_f = [':(','=(', ':c', ':C', '☹️', '😢', '😭', '🙁', '😟', '😒', '😔', '📉','💀']\n\ndef apply_filter(data):\n    positive = data[data['Text'].str.contains('|'.join(positive_text))]['Text'].tolist()\n    negative = data[data['Text'].str.contains('|'.join(negative_text))]['Text'].tolist()\n    \n    return positive, negative"},{"cell_type":"code","execution_count":5,"id":"a609ff23-5c3e-4c67-96ac-796d81f55666","metadata":{},"outputs":[],"source":"def get_positive_negative_data(date_from, date_until):\n    end_positive, end_negative = [], []\n    total_number = 0\n    \n    while date_from < date_until:\n        \n        if date_from.day == 1 and date_from.month == 1:\n            print(f\"Current Date {str(date_from)}\")\n        \n        folder = os.path.join(t_path, str(date_from))\n        \n        tweet_file = os.path.join(folder, t_file)\n        tweet_df = pd.read_csv(tweet_file, sep=\";\")\n        \n        total_number += len(tweet_df)\n        \n        positive, negative = apply_filter(tweet_df)\n        \n        end_positive += positive\n        end_negative += negative\n        \n        date_from = date_from + timedelta(days=1)\n    \n    return end_positive, end_negative, total_number"},{"cell_type":"code","execution_count":6,"id":"5ec7d24e-ff8a-4dcb-afae-2f4442d0bd7f","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Current Date 2014-01-01\n","Extraction Completed!\n"]}],"source":"positive, negative, df_len = get_positive_negative_data(date_from, date_until)\nprint(\"Extraction Completed!\")"},{"cell_type":"code","execution_count":7,"id":"f740ebc9-53b4-41a4-9176-961e177bec89","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["15349\n","3949\n","---\n","19298\n"]}],"source":"print(len(positive))\nprint(len(negative))\nprint(\"---\")\nprint(len(positive) + len(negative))"},{"cell_type":"code","execution_count":8,"id":"3daa654d-31a4-4926-b7ae-da0e9d778224","metadata":{},"outputs":[],"source":"def remove_emojis(data, filters = [positive_text_f, negative_text_f]):\n    ''' Removes the emojis from the sentences '''\n    for i, sentence in enumerate(data):\n        for i_filter in filters:\n            for element in i_filter:\n                sentence =  sentence.replace(element, \"\")\n        data[i] = sentence\n    return data"},{"cell_type":"markdown","id":"23630653-b418-4f71-b1d6-1083e0a99889","metadata":{},"source":"## **Metric and Distance**\n\nThe idea of the spam filtering, is using a metric to detect highly repeated messages on a big sample. JABA has a built in method based on DBSCAN from sklearn to use it. \n\nThis method can be found in JABA.service.scraper.spam.filtering.filter_spam.\n\nThe method divides the whole dataset into batches of *batch_size* (5000 as default) size and then tries to cluster them. The ones clustered are the one selected as spam."},{"cell_type":"code","execution_count":9,"id":"71f61fca-4f6b-45aa-ac96-042487828404","metadata":{},"outputs":[],"source":"from JABA.service.scraper.spam import filtering, metrics"},{"cell_type":"code","execution_count":19,"id":"d84d098d-1d5a-4d84-a975-fd34d0d09e5c","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Current batch 15 of 15. ETA : 4: 965\r"]}],"source":"end_positive = filter_spam(positive, verbose = True)\nend_negative = filter_spam(negative, verbose = True)"},{"cell_type":"code","execution_count":20,"id":"fcec8113-fc64-4c88-b2df-dae289999196","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["358465\n","281398\n","0.78500829927608\n","----------\n","74601\n","0.7715982359485798\n"]}],"source":"print(len(positive))\nprint(len(end_positive))\nprint(len(end_positive)/len(positive))\nprint(\"-\"*10)\nprint(len(negative))\nprint(len(end_negative)/len(negative))"},{"cell_type":"code","execution_count":21,"id":"9a8b05bc-5774-4450-8984-c9082b21b3f1","metadata":{},"outputs":[],"source":"all_sentence = remove_emojis(end_positive + end_negative)\nall_sentiment = [1] * len(end_positive) + [0] * len(end_negative)\nall_map = {'text':all_sentence, 'sentiment':all_sentiment} \nfinal_df = pd.DataFrame(all_map)\nfinal_df.to_csv('filter_sentiment.csv')"}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}