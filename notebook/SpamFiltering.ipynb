{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b9bef79",
   "metadata": {},
   "source": [
    "# **Global Sentiment**\n",
    "\n",
    "En este cuaderno mostraremos la forma de extraer el *Sentimiento Global*, mÃ©trica que hemos desarrollado para este proyecto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d891a12-ed02-4d20-8861-e0f8d7f31bf2",
   "metadata": {},
   "source": [
    "## Imports y Parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdb93129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "import snscrape.modules.twitter as snstwitter\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3216cd1-6de3-420d-9707-a9cc4633e4a8",
   "metadata": {},
   "source": [
    "Podemos escoger las fechas que queremos analizar. Para poder analizarlas, debemos tener los archivos de las fechas correspondientes descargados, sino no podremos ejecutar el analisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7be423b0-c422-41cd-add6-069aa9ce1886",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_init = \"2014-01-01\"\n",
    "date_limit = \"2018-05-30\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a3fab3-aae3-45dc-ab9d-49f52eed160b",
   "metadata": {},
   "source": [
    "## **Read Databases**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b9ac58-8e46-4747-a34f-0b09acad857e",
   "metadata": {},
   "source": [
    "Definimos la carpeta donde se encuentran los datos asÃ­ como los nombres de los archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55d9f863-6f77-4b42-a164-e7ee56a677cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_path = \"../JABA/data/tweets\"\n",
    "t_file = \"tweet_list.csv\"\n",
    "s_file = \"tweet_sentiment_nltk.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db108986-70a6-4b79-9d23-110ab8cbfa16",
   "metadata": {},
   "source": [
    "La base de datos esta formada por millones de filas y no usaremos todas las columnas, por lo que, para acelerar el proceso, eliminaremos las columnas no usadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a609ff23-5c3e-4c67-96ac-796d81f55666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_data():\n",
    "    frames = []\n",
    "    date_from = datetime.datetime.strptime(date_init, '%Y-%m-%d').date()\n",
    "    date_until = datetime.datetime.strptime(date_limit, '%Y-%m-%d').date()\n",
    "    \n",
    "    if date_from >= date_until:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    while date_from < date_until:\n",
    "        \n",
    "        folder = os.path.join(t_path, str(date_from))\n",
    "        # TODO Check if file exists\n",
    "        if date_from.day == 1 and date_from.month == 1:\n",
    "            print(f\"Current Date {str(date_from)}\")\n",
    "\n",
    "        tweet_file = os.path.join(folder, t_file)\n",
    "        sentiment_file = os.path.join(folder, s_file)\n",
    "\n",
    "        tweet_df = pd.read_csv(tweet_file, sep=\";\")\n",
    "\n",
    "        sent_df = pd.read_csv(sentiment_file, sep=\";\")\n",
    "        \n",
    "        frames += [tweet_df]\n",
    "        \n",
    "        date_from = date_from + timedelta(days=1)\n",
    "    \n",
    "    return pd.concat(frames, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ec7d24e-ff8a-4dcb-afae-2f4442d0bd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Date 2014-01-01\n",
      "Current Date 2015-01-01\n",
      "Current Date 2016-01-01\n",
      "Current Date 2017-01-01\n",
      "Current Date 2018-01-01\n",
      "Extraction Completed!\n"
     ]
    }
   ],
   "source": [
    "df = get_all_data()\n",
    "print(\"Extraction Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f92bb857-1250-466c-add2-c4e299af445d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18477101"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94a33757-537e-46a2-b12c-c2c4277319e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_text = [':\\)', ':D', '=D','=\\)','ðŸ˜Š', 'ðŸš€', 'ðŸ”¥','ðŸ˜‹', 'ðŸ’°', 'ðŸ“ˆ','ðŸ’¯']\n",
    "negative_text = [':\\(','=\\(', ':c', ':C', 'â˜¹ï¸', 'ðŸ˜¢', 'ðŸ˜­', 'ðŸ™', 'ðŸ˜Ÿ', 'ðŸ˜’', 'ðŸ˜”','ðŸ“‰','ðŸ’€']\n",
    "\n",
    "positive_text_f = [':)', ':D', '=D','=)','ðŸ˜Š', 'ðŸš€', 'ðŸ”¥','ðŸ˜‹', 'ðŸ’°', 'ðŸ“ˆ','ðŸ’¯']\n",
    "negative_text_f = [':(','=(', ':c', ':C', 'â˜¹ï¸', 'ðŸ˜¢', 'ðŸ˜­', 'ðŸ™', 'ðŸ˜Ÿ', 'ðŸ˜’', 'ðŸ˜”', 'ðŸ“‰','ðŸ’€']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efc6c272-9fff-4d0a-b6d8-6eec5451e06c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "positive = df[df['Text'].str.contains('|'.join(positive_text))]['Text'].tolist()\n",
    "negative = df[df['Text'].str.contains('|'.join(negative_text))]['Text'].tolist()\n",
    "all_sent = df[df['Text'].str.contains('|'.join(positive_text+negative_text))]['Text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f740ebc9-53b4-41a4-9176-961e177bec89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358465\n",
      "74601\n",
      "---\n",
      "415587\n",
      "433066\n"
     ]
    }
   ],
   "source": [
    "print(len(positive))\n",
    "print(len(negative))\n",
    "print(\"---\")\n",
    "print(len(all_sent))\n",
    "print(len(positive) + len(negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3daa654d-31a4-4926-b7ae-da0e9d778224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(data, filters = [positive_text_f, negative_text_f]):\n",
    "    ''' Removes the emojis from the sentences '''\n",
    "    for i, sentence in enumerate(data):\n",
    "        for i_filter in filters:\n",
    "            for element in i_filter:\n",
    "                sentence =  sentence.replace(element, \"\")\n",
    "        data[i] = sentence\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23630653-b418-4f71-b1d6-1083e0a99889",
   "metadata": {},
   "source": [
    "## **Metric and Distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71f61fca-4f6b-45aa-ac96-042487828404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24624aed-5568-4fc8-9781-2cb07828194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacard_t(txt1, txt2):\n",
    "    words1 = set(txt1.split(' '))\n",
    "    words2 = set(txt2.split(' '))\n",
    "    return 1 - len(words1.intersection(words2)) / len(words1.union(words2))\n",
    "\n",
    "def jacard(index1, index2):\n",
    "    words1 = set(positive[int(index1)].split(' '))\n",
    "    words2 = set(positive[int(index2)].split(' '))\n",
    "    return 1 - len(words1.intersection(words2)) / len(words1.union(words2))\n",
    "\n",
    "def soronsen(index1, index2):\n",
    "    words1 = set(positive[int(index1)].split(' '))\n",
    "    words2 = set(positive[int(index2)].split(' '))\n",
    "    return 1 - 2 * len(words1.intersection(words2)) / ( len(words1) + len(words2))\n",
    "\n",
    "def overlap(index1, index2):\n",
    "    words1 = set(positive[int(index1)].split(' '))\n",
    "    words2 = set(positive[int(index2)].split(' '))\n",
    "    return 1 - len(words1.intersection(words2)) / ( min( len(words1), len(words2) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbd19fb9-3bb7-45b7-8d64-74276e2f3d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_spam(data, batch_size = 5000, verbose = False, metric = jacard, eps = 0.3):\n",
    "    ''' Filters spam based on text similarity '''\n",
    "    batches = ceil(len(data)/batch_size)\n",
    "\n",
    "    filtered_data = []\n",
    "\n",
    "    if verbose:\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "    for n_batch in range(batches):\n",
    "        if verbose:\n",
    "            last_batch_time =  time.time() - start_time\n",
    "            start_time = time.time()\n",
    "            \n",
    "            eta_time = (batches + 1 - n_batch ) * last_batch_time \n",
    "            \n",
    "            batch_output = f'Current batch {n_batch+1} of {batches}.' \n",
    "            time_output = 'ETA : %i:%2i' % ( eta_time//60, int(eta_time)%60 )\n",
    "            print(batch_output + ' ' + time_output , end='\\r')\n",
    "            \n",
    "        if n_batch == batches - 1:\n",
    "            batch = data[batch_size * n_batch:]\n",
    "        else:\n",
    "            batch = data[batch_size * n_batch:batch_size * (n_batch+1)]\n",
    "\n",
    "        X = np.arange(batch_size * n_batch,  batch_size * n_batch + len(batch)).reshape(-1, 1)\n",
    "        db = DBSCAN(eps=eps,  metric=metric).fit(X)\n",
    "                    \n",
    "        for i,v in enumerate(db.labels_):\n",
    "            if v == -1:\n",
    "                filtered_data += [batch[i]]\n",
    "                \n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84d098d-1d5a-4d84-a975-fd34d0d09e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current batch 47 of 72. ETA : 92:215\r"
     ]
    }
   ],
   "source": [
    "end_positive = filter_spam(positive, verbose = True)\n",
    "end_negative = filter_spam(negative, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcec8113-fc64-4c88-b2df-dae289999196",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(positive))\n",
    "print(len(end_positive))\n",
    "print(len(end_positive)/len(positive))\n",
    "print(\"-\"*10)\n",
    "print(len(negative))\n",
    "print(len(end_negative)/len(negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8b05bc-5774-4450-8984-c9082b21b3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentence = remove_emojis(end_positive + end_negative)\n",
    "all_sentiment = [1] * len(end_positive) + [0] * len(end_negative)\n",
    "all_map = {'text':all_sentence, 'sentiment':all_sentiment} \n",
    "final_df = pd.DataFrame(all_map)\n",
    "final_df.to_csv('filter_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965f8965-f952-48d1-beba-5ab743619fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentence[17960:18000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69891de-af26-419c-9942-5e3778d042f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
