{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b9bef79",
   "metadata": {},
   "source": [
    "# **Global Sentiment**\n",
    "\n",
    "En este cuaderno mostraremos la forma de extraer el *Sentimiento Global*, métrica que hemos desarrollado para este proyecto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d891a12-ed02-4d20-8861-e0f8d7f31bf2",
   "metadata": {},
   "source": [
    "## Imports y Parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdb93129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "import snscrape.modules.twitter as snstwitter\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3216cd1-6de3-420d-9707-a9cc4633e4a8",
   "metadata": {},
   "source": [
    "Podemos escoger las fechas que queremos analizar. Para poder analizarlas, debemos tener los archivos de las fechas correspondientes descargados, sino no podremos ejecutar el analisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7be423b0-c422-41cd-add6-069aa9ce1886",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_init = \"2014-01-01\"\n",
    "date_limit = \"2016-08-01\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a3fab3-aae3-45dc-ab9d-49f52eed160b",
   "metadata": {},
   "source": [
    "## **Read Databases**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b9ac58-8e46-4747-a34f-0b09acad857e",
   "metadata": {},
   "source": [
    "Definimos la carpeta donde se encuentran los datos así como los nombres de los archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55d9f863-6f77-4b42-a164-e7ee56a677cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_path = \"../JABA/data/tweets\"\n",
    "t_file = \"tweet_list.csv\"\n",
    "s_file = \"tweet_sentiment_nltk.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db108986-70a6-4b79-9d23-110ab8cbfa16",
   "metadata": {},
   "source": [
    "La base de datos esta formada por millones de filas y no usaremos todas las columnas, por lo que, para acelerar el proceso, eliminaremos las columnas no usadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a609ff23-5c3e-4c67-96ac-796d81f55666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_data():\n",
    "    frames = []\n",
    "    date_from = datetime.datetime.strptime(date_init, '%Y-%m-%d').date()\n",
    "    date_until = datetime.datetime.strptime(date_limit, '%Y-%m-%d').date()\n",
    "    \n",
    "    if date_from >= date_until:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    while date_from < date_until:\n",
    "        \n",
    "        folder = os.path.join(t_path, str(date_from))\n",
    "        # TODO Check if file exists\n",
    "        if date_from.day == 1 and date_from.month == 1:\n",
    "            print(f\"Current Date {str(date_from)}\")\n",
    "\n",
    "        tweet_file = os.path.join(folder, t_file)\n",
    "        sentiment_file = os.path.join(folder, s_file)\n",
    "\n",
    "        tweet_df = pd.read_csv(tweet_file, sep=\";\")\n",
    "\n",
    "        sent_df = pd.read_csv(sentiment_file, sep=\";\")\n",
    "        \n",
    "        frames += [tweet_df]\n",
    "        \n",
    "        date_from = date_from + timedelta(days=1)\n",
    "    \n",
    "    return pd.concat(frames, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ec7d24e-ff8a-4dcb-afae-2f4442d0bd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Date 2014-01-01\n",
      "Current Date 2015-01-01\n",
      "Current Date 2016-01-01\n",
      "Extraction Completed!\n"
     ]
    }
   ],
   "source": [
    "df = get_all_data()\n",
    "print(\"Extraction Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f92bb857-1250-466c-add2-c4e299af445d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6923270"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94a33757-537e-46a2-b12c-c2c4277319e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_text = [':\\)', ':D', '=D','=\\)','😊', '🚀', '🔥','😋', '💰', '📈','💯']\n",
    "negative_text = [':\\(','=\\(', ':c', ':C', '☹️', '😢', '😭', '🙁', '😟', '😒', '😔','📉','💀']\n",
    "\n",
    "positive_text_f = [':)', ':D', '=D','=)','😊', '🚀', '🔥','😋', '💰', '📈','💯']\n",
    "negative_text_f = [':(','=(', ':c', ':C', '☹️', '😢', '😭', '🙁', '😟', '😒', '😔', '📉','💀']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efc6c272-9fff-4d0a-b6d8-6eec5451e06c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "positive = df[df['Text'].str.contains('|'.join(positive_text))]['Text'].tolist()\n",
    "negative = df[df['Text'].str.contains('|'.join(negative_text))]['Text'].tolist()\n",
    "all_sent = df[df['Text'].str.contains('|'.join(positive_text+negative_text))]['Text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f740ebc9-53b4-41a4-9176-961e177bec89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64886\n",
      "28127\n",
      "---\n",
      "91797\n",
      "93013\n"
     ]
    }
   ],
   "source": [
    "print(len(positive))\n",
    "print(len(negative))\n",
    "print(\"---\")\n",
    "print(len(all_sent))\n",
    "print(len(positive) + len(negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70a1f3e6-c15e-4a55-8e5e-d5888b56a497",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentence = positive + negative\n",
    "\n",
    "for i, sentence in enumerate(all_sentence):\n",
    "    for element in positive_text_f:\n",
    "        sentence =  sentence.replace(element, \"\")\n",
    "    for element in negative_text_f:\n",
    "        sentence =  sentence.replace(element, \"\")\n",
    "        \n",
    "    all_sentence[i] = sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bab7101e-b4fb-492e-a9b0-a859fc0ebaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_sentiment = [1] * len(positive) + [0] * len(negative)\n",
    "all_map = {'text':all_sentence, 'sentiment':all_sentiment} \n",
    "final_df = pd.DataFrame(all_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "658a1a0d-0753-40a4-b85c-c5fb40015a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('sentiment.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23630653-b418-4f71-b1d6-1083e0a99889",
   "metadata": {},
   "source": [
    "## **Metric and Distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71f61fca-4f6b-45aa-ac96-042487828404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24624aed-5568-4fc8-9781-2cb07828194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacard_t(txt1, txt2):\n",
    "    words1 = set(txt1.split(' '))\n",
    "    words2 = set(txt2.split(' '))\n",
    "    return 1 - len(words1.intersection(words2)) / len(words1.union(words2))\n",
    "\n",
    "def jacard(index1, index2):\n",
    "    words1 = set(positive[int(index1)].split(' '))\n",
    "    words2 = set(positive[int(index2)].split(' '))\n",
    "    return 1 - len(words1.intersection(words2)) / len(words1.union(words2))\n",
    "\n",
    "def soronsen(index1, index2):\n",
    "    words1 = set(positive[int(index1)].split(' '))\n",
    "    words2 = set(positive[int(index2)].split(' '))\n",
    "    return 1 - 2 * len(words1.intersection(words2)) / ( len(words1) + len(words2))\n",
    "\n",
    "def overlap(index1, index2):\n",
    "    words1 = set(positive[int(index1)].split(' '))\n",
    "    words2 = set(positive[int(index2)].split(' '))\n",
    "    return 1 - len(words1.intersection(words2)) / ( min( len(words1), len(words2) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbd19fb9-3bb7-45b7-8d64-74276e2f3d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_spam(data, batch_size = 5000, verbose = False, metric = jacard, eps = 0.3):\n",
    "    ''' Filters spam based on text similarity '''\n",
    "    batches = ceil(len(data)/batch_size)\n",
    "\n",
    "    filtered_data = []\n",
    "\n",
    "    if verbose:\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "    for n_batch in range(batches):\n",
    "        if verbose:\n",
    "            last_batch_time =  time.time() - start_time\n",
    "            start_time = time.time()\n",
    "            \n",
    "            eta_time = (batches + 1 - n_batch ) * last_batch_time \n",
    "            \n",
    "            print(f\"Current batch {n_batch} of {batches}\")\n",
    "            \n",
    "            print(\"ETA: %i:%i\" % ( eta_time//60, int(eta_time)%60 ) )\n",
    "            \n",
    "        if n_batch == batches - 1:\n",
    "            batch = data[batch_size * n_batch:]\n",
    "        else:\n",
    "            batch = data[batch_size * n_batch:batch_size * (n_batch+1)]\n",
    "\n",
    "        X = np.arange(batch_size * n_batch,  batch_size * n_batch + len(batch)).reshape(-1, 1)\n",
    "        db = DBSCAN(eps=eps,  metric=metric).fit(X)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Number of batch labels found {max(db.labels_)}\")\n",
    "            \n",
    "        for i,v in enumerate(db.labels_):\n",
    "            if v == -1:\n",
    "                filtered_data += [batch[i]]\n",
    "                \n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d84d098d-1d5a-4d84-a975-fd34d0d09e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current batch 0 of 13\n",
      "ETA: 0:0\n",
      "Number of batch labels found 4\n",
      "Current batch 1 of 13\n",
      "ETA: 26:4\n",
      "Number of batch labels found 5\n",
      "Current batch 2 of 13\n",
      "ETA: 24:41\n",
      "Number of batch labels found 17\n",
      "Current batch 3 of 13\n",
      "ETA: 23:24\n",
      "Number of batch labels found 14\n",
      "Current batch 4 of 13\n",
      "ETA: 21:6\n",
      "Number of batch labels found 19\n",
      "Current batch 5 of 13\n",
      "ETA: 18:38\n",
      "Number of batch labels found 34\n",
      "Current batch 6 of 13\n",
      "ETA: 16:35\n",
      "Number of batch labels found 27\n",
      "Current batch 7 of 13\n",
      "ETA: 14:32\n",
      "Number of batch labels found 25\n",
      "Current batch 8 of 13\n",
      "ETA: 12:23\n",
      "Number of batch labels found 33\n",
      "Current batch 9 of 13\n",
      "ETA: 10:11\n",
      "Number of batch labels found 42\n",
      "Current batch 10 of 13\n",
      "ETA: 8:0\n",
      "Number of batch labels found 46\n",
      "Current batch 11 of 13\n",
      "ETA: 6:5\n",
      "Number of batch labels found 55\n",
      "Current batch 12 of 13\n",
      "ETA: 4:0\n",
      "Number of batch labels found 47\n",
      "Current batch 0 of 6\n",
      "ETA: 0:0\n",
      "Number of batch labels found 4\n",
      "Current batch 1 of 6\n",
      "ETA: 11:40\n",
      "Number of batch labels found 5\n",
      "Current batch 2 of 6\n",
      "ETA: 10:2\n",
      "Number of batch labels found 17\n",
      "Current batch 3 of 6\n",
      "ETA: 8:26\n",
      "Number of batch labels found 14\n",
      "Current batch 4 of 6\n",
      "ETA: 6:29\n",
      "Number of batch labels found 19\n",
      "Current batch 5 of 6\n",
      "ETA: 4:10\n",
      "Number of batch labels found 15\n"
     ]
    }
   ],
   "source": [
    "end_positive = filter_spam(positive, verbose = True)\n",
    "end_negative = filter_spam(negative, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "67cce4af-80c7-48f9-a0b6-72df9a078df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n",
      "r/btc mod logs public, but posting/discussing the history censored/not allowed :) bitcoin today!\n",
      "r/btc mod logs public, but posting/discussing the history censored/not allowed :): bitcoin btc\n",
      "r/btc mod logs public, but posting/discussing the history censored/not allowed :) bitcoin blockchain\n",
      "r/btc mod logs public, but posting/discussing the history censored/not allowed :) bitcoin blockchain cryptos r…\n",
      "r/btc mod logs public, but posting/discussing the history censored/not allowed :) (via /r/bitcoin)\n"
     ]
    }
   ],
   "source": [
    "n_b = 12\n",
    "print(max(tot[n_b][0]))\n",
    "for i in range(0, len(tot[n_b][0])):\n",
    "    if tot[n_b][0][i] == 45:\n",
    "        print(positive[batch_size*n_b+i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcec8113-fc64-4c88-b2df-dae289999196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64886\n",
      "51737\n",
      "0.7973522793823012\n",
      "----------\n",
      "28127\n",
      "0.8041383723824084\n"
     ]
    }
   ],
   "source": [
    "print(len(positive))\n",
    "print(len(end_positive))\n",
    "print(len(end_positive)/len(positive))\n",
    "print(\"-\"*10)\n",
    "print(len(negative))\n",
    "print(len(end_negative)/len(negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a8b05bc-5774-4450-8984-c9082b21b3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentence = end_positive + end_negative\n",
    "all_sentiment = [1] * len(end_positive) + [0] * len(end_negative)\n",
    "all_map = {'text':all_sentence, 'sentiment':all_sentiment} \n",
    "final_df = pd.DataFrame(all_map)\n",
    "final_df.to_csv('filter_sentiment.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882e8b05-22ca-4816-8e8f-515d1fd9d9b9",
   "metadata": {},
   "source": [
    "## **Tensorflow Fine Tuning**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc24c56e-9f41-4a4f-a07d-6ba72b0ee845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a25f489-0c8e-42ff-8e2b-4de612599554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "GPU device not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-afda44667708>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Found GPU at: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GPU device not found'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mSystemError\u001b[0m: GPU device not found"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Get the GPU device name.\n",
    "device_name = tf.test.gpu_device_name()\n",
    "print(device_name)\n",
    "# The device name should look like the following:\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af6dfc49-d3eb-44e7-a428-4b4bfacb0cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51150c49-b087-4d22-8f7e-e1e2236c8919",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews', \n",
    "          split = (tfds.Split.TRAIN, tfds.Split.TEST),\n",
    "          as_supervised=True,\n",
    "          with_info=True)\n",
    "print('info', ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "201a815d-578f-4495-af81-f59ca2e4aae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01bd79df6a0f4987abce9f9e82a11e36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7373bc18ee034ba79ee726f0c4f52335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af00fbae-6cc6-4047-b4aa-672b1bdd2629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['knight', 'lap', 'survey', 'ma', '##ow', 'noise', 'billy', '##ium', 'shooting', 'guide', 'bedroom', 'priest', 'resistance', 'motor', 'homes', 'sounded', 'giant', '##mer', '150', 'scenes']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = tokenizer.get_vocab()\n",
    "\n",
    "print(list(vocabulary.keys())[5000:5020])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0545f2b-c954-4162-8e39-83863b622c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized ['[CLS]', 'test', 'token', '##ization', 'sentence', '.', 'followed', 'by', 'another', 'sentence', '[SEP]']\n",
      "{'token_ids': [101, 3231, 19204, 3989, 6251, 1012, 2628, 2011, 2178, 6251, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "max_length_test = 20\n",
    "test_sentence = 'Test tokenization sentence. Followed by another sentence'\n",
    "\n",
    "# add special tokens\n",
    "\n",
    "test_sentence_with_special_tokens = '[CLS]' + test_sentence + '[SEP]'\n",
    "\n",
    "tokenized = tokenizer.tokenize(test_sentence_with_special_tokens)\n",
    "\n",
    "print('tokenized', tokenized)\n",
    "\n",
    "# convert tokens to ids in WordPiece\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "  \n",
    "# precalculation of pad length, so that we can reuse it later on\n",
    "padding_length = max_length_test - len(input_ids)\n",
    "\n",
    "# map tokens to WordPiece dictionary and add pad token for those text shorter than our max length\n",
    "input_ids = input_ids + ([0] * padding_length)\n",
    "\n",
    "# attention should focus just on sequence with non padded tokens\n",
    "attention_mask = [1] * len(input_ids)\n",
    "\n",
    "# do not focus attention on padded tokens\n",
    "attention_mask = attention_mask + ([0] * padding_length)\n",
    "\n",
    "# token types, needed for example for question answering, for our purpose we will just set 0 as we have just one sequence\n",
    "token_type_ids = [0] * max_length_test\n",
    "\n",
    "bert_input = {\n",
    "    \"token_ids\": input_ids,\n",
    "    \"token_type_ids\": token_type_ids,\n",
    "    \"attention_mask\": attention_mask\n",
    "}\n",
    "print(bert_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66d1027a-8d07-4884-ad5c-f597b1a62d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded {'input_ids': [101, 3231, 19204, 3989, 6251, 1012, 2628, 2011, 2178, 6251, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\whiwho\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\tokenization_utils_base.py:2126: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "bert_input = tokenizer.encode_plus(\n",
    "                        test_sentence,                      \n",
    "                        add_special_tokens = True, # add [CLS], [SEP]\n",
    "                        max_length = max_length_test, # max length of the text that can go to BERT\n",
    "                        pad_to_max_length = True, # add [PAD] tokens\n",
    "                        return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
    "              )\n",
    "\n",
    "print('encoded', bert_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9813f9f-f1e5-49fa-b01a-9aa97bc08ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# can be up to 512 for BERT\n",
    "max_length = 512\n",
    "batch_size = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bea2230-e5af-43e2-a056-86e7a29b6c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review This was an absolutely terrible movie. Don't be lu 0\n",
      "review I have been known to fall asleep during films, but 0\n",
      "review Mann photographs the Alberta Rocky Mountains in a  0\n",
      "review This is the kind of film for a snowy Sunday aftern 1\n",
      "review As others have mentioned, all the women that go nu 1\n"
     ]
    }
   ],
   "source": [
    "for review, label in tfds.as_numpy(ds_train.take(5)):\n",
    "    print('review', review.decode()[0:50], label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35c57026-54d8-4d6b-a78b-583811159e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example_to_feature(review):\n",
    "  \n",
    "  # combine step for tokenization, WordPiece vector mapping, adding special tokens as well as truncating reviews longer than the max length\n",
    "  \n",
    "  return tokenizer.encode_plus(review, \n",
    "                add_special_tokens = True, # add [CLS], [SEP]\n",
    "                max_length = max_length, # max length of the text that can go to BERT\n",
    "                pad_to_max_length = True, # add [PAD] tokens\n",
    "                return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
    "              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6f2a76a-74df-4ca5-8cd3-8a6e7c3ace14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"token_type_ids\": token_type_ids,\n",
    "        \"attention_mask\": attention_masks,\n",
    "    }, label\n",
    "def encode_examples(ds, limit=-1):\n",
    "    # prepare list, so that we can build up final TensorFlow dataset from slices.\n",
    "    input_ids_list = []\n",
    "    token_type_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    label_list = []\n",
    "    if (limit > 0):\n",
    "        ds = ds.take(limit)\n",
    "    \n",
    "    for review, label in tfds.as_numpy(ds):\n",
    "        bert_input = convert_example_to_feature(review.decode())\n",
    "\n",
    "        input_ids_list.append(bert_input['input_ids'])\n",
    "        token_type_ids_list.append(bert_input['token_type_ids'])\n",
    "        attention_mask_list.append(bert_input['attention_mask'])\n",
    "        label_list.append([label])\n",
    "    return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52fb35be-8057-4a6b-bc86-21fe4033dc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset\n",
    "ds_train_encoded = encode_examples(ds_train).shuffle(10000).batch(batch_size)\n",
    "# test dataset\n",
    "ds_test_encoded = encode_examples(ds_test).batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb2cbc2-00f5-4e02-9df5-7d120dc20d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c0d3372-36a4-4201-8282-8457dac806f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "# recommended learning rate for Adam 5e-5, 3e-5, 2e-5\n",
    "learning_rate = 2e-5\n",
    "\n",
    "# we will do just 1 epoch for illustration, though multiple epochs might be better as long as we will not overfit the model\n",
    "number_of_epochs = 1\n",
    "\n",
    "# model initialization\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# optimizer Adam\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
    "\n",
    "# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "134af46a-264c-4ac3-b413-82765cabda16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x0000014AC148E9A0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x0000014AC148E9A0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x0000014AC148E9A0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\whiwho\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\whiwho\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  23/4167 [..............................] - ETA: 11:24:19 - loss: 0.7153 - accuracy: 0.4783"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-7e871179b930>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbert_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_train_encoded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumber_of_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mds_test_encoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bert_history = model.fit(ds_train_encoded, epochs=number_of_epochs, validation_data=ds_test_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784eff4f-5f0b-4182-afae-f685d9016f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
